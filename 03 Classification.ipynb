{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split, ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import auc, roc_auc_score, accuracy_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from util.isoelectric_point import isoelectric_points\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "There are influenza viruses that are collected from the \"environment\", or have an \"unknown\" host. How do we infer which hosts it came from? Well, that sounds like a **Classification** problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the sequences into memory\n",
    "sequences = [s for s in SeqIO.parse('data/20160127_HA_prediction.fasta', 'fasta') if len(s.seq) == 566]  # we are cheating and not bothering with an alignment.\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the sequence IDs into memory\n",
    "seqids = [s.id for s in SeqIO.parse('data/20160127_HA_prediction.fasta', 'fasta') if len(s.seq) == 566]\n",
    "len(seqids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cast the sequences as a MultipleSeqAlignment object, and then turn that into a pandas DataFrame. \n",
    "# Note: this cell takes a while.\n",
    "seq_aln = MultipleSeqAlignment(sequences)\n",
    "seq_df = pd.DataFrame(np.array(seq_aln))\n",
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the df into isoelectric point features.\n",
    "seq_feats = seq_df.replace(isoelectric_points.keys(), isoelectric_points.values())\n",
    "seq_feats.index = seqids\n",
    "seq_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quick check to make sure that we have no strings:\n",
    "for c in seq_feats.columns:\n",
    "    letters = set(seq_feats[c])\n",
    "    for item in letters:\n",
    "        assert not isinstance(item, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us now load our labels.\n",
    "labels = pd.read_csv('data/20160127_HA_prediction.csv', parse_dates=['Collection Date'])\n",
    "labels['Host Species'] = labels['Host Species'].str.replace('IRD:', '').str.replace('/Avian', '')\n",
    "labels['Sequence Accession'] = labels['Sequence Accession'].str.replace('*', '')\n",
    "labels.set_index('Sequence Accession', inplace=True)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's join in the labels so that we have everything in one big massive table.\n",
    "data_matrix = seq_feats.join(labels['Host Species'], how='inner')\n",
    "data_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quickly inspect the different labels under \"host species\"\n",
    "# set(data_matrix['Host Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will want to predict the labels for: \"Avian\", \"Bird\", \"Environment\", \"Unknown\", \"null\"\n",
    "unknown_labels = ['Avian', 'Bird', 'Environment', 'Unknown', 'null']\n",
    "known_labels = set(data_matrix['Host Species']) - set(unknown_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's further split the data into the \"unknowns\" and the \"knowns\"\n",
    "unknowns = data_matrix[data_matrix['Host Species'].isin(unknown_labels)]\n",
    "knowns = data_matrix[data_matrix['Host Species'].isin(known_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we want to convert the known host species into a matrix of 1s and 0s, so that we can use them as inputs\n",
    "# to the training algorithm.\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit([s for s in known_labels])\n",
    "lb.transform(knowns['Host Species'])  # note: this has not done anything to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chalk Talk Time!\n",
    "\n",
    "We're almost ready for training a machine learning model to classify the unknown hosts based on their sequence.\n",
    "\n",
    "Here's the proper procedure.\n",
    "\n",
    "1. Split the labelled data into a training and testing set. (~70 train/30 test to 80 train/20 test)\n",
    "1. Train and evaluate a model on the training set.\n",
    "1. Make predictions on the testing set, evaluate the model on testing set accuracy.\n",
    "\n",
    "This procedure is known as cross-validation, and is a powerful, yet cheap & easy method for evaluating how good a particular supervised learning model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into a training and testing set.\n",
    "X_cols = [i for i in range(0,566)]\n",
    "X = knowns[X_cols]\n",
    "Y = lb.transform(knowns['Host Species'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier.\n",
    "# Note: This cell takes a while; any questions?\n",
    "\n",
    "# Initialize the classifier object.\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Train (i.e. \"fit\") the classifier to the training Xs and Ys\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test X\n",
    "preds = clf.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we evaluate how good the classification task performed?\n",
    "\n",
    "For binary classification, the Receiver-Operator Characteristic curve is a great way to evaluate a classification task.\n",
    "\n",
    "For multi-label classification, which is the case we have here, accuracy score is a great starting place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's first take a look at the accuracy score: the fraction that were classified correctly.\n",
    "accuracy_score(lb.inverse_transform(Y_test), lb.inverse_transform(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about those sequences for which the hosts were unknown?\n",
    "\n",
    "We can run the `predict(unknown_Xs)` to predict what their hosts were likely to be, given their sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unknown_preds = clf.predict(unknowns[X_cols])\n",
    "unknown_preds = lb.inverse_transform(unknown_preds)\n",
    "unknown_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this gives us is the class label with the highest probability of being the correct one. \n",
    "\n",
    "While we will not do this here, at this point, it would be a good idea to double-check your work with a sanity check. Are the sequences that are predicted to be `Human` truly of a close sequence similarity to actual `Human` sequences? You may want to do a Multiple Sequence Alignment, or you might want to simply compute the Levenshtein or Hamming distance between the two sequences, as a sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we interpret what the classifier learned?\n",
    "\n",
    "Depending on the classifier used, you can peer inside the model to get a feel for what the classifier learned about the features that best predict the class label.\n",
    "\n",
    "The `RandomForestClassifier` provides a `feature_importances_` attribute that we can access and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation here is that the positions with greater \"feature importance\" were better at predicting the host class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercises\n",
    "\n",
    "Now that you've seen the coding pattern for classification using the `RandomForestClassifier`, try it out for the `DecisionTreeClassifier`, evaluate its accuracy, and if applicable, try doing the \"peering inside\" of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- Classification: it's all about using features to predict a label.\n",
    "    - `classifier.fit(train_data)`\n",
    "    - `classifier.predict(test_data)`\n",
    "- Split your data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercise (time permitting)\n",
    "\n",
    "Can you train a model to classify the two types of influenza H1N1 sequences from the previous notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
